import spacy
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
from collections import Counter

# Load spaCy models
nlp_sci = spacy.load('en_core_sci_sm')
nlp_bc5cdr = spacy.load('en_ner_bc5cdr_md')

# Load BioBERT model and tokenizer
biobert_model_name = 'dmis-lab/biobert-base-cased-v1.1'
biobert_tokenizer = AutoTokenizer.from_pretrained(biobert_model_name)
biobert_model = AutoModelForTokenClassification.from_pretrained(biobert_model_name)

# Function to extract entities using spaCy
def extract_entities_spacy(text, model):
    doc = model(text)
    entities = [ent.text for ent in doc.ents]
    return entities

# Function to extract entities using BioBERT
def extract_entities_biobert(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=2)
    
    id2label = {i: label for i, label in enumerate(tokenizer.convert_ids_to_tokens(predictions[0].tolist()))}
    
    entities = []
    current_entity = ''
    
    for token, prediction in zip(tokenizer.tokenize(text, return_tensors="pt", truncation=True, padding=True), predictions[0].tolist()):
        label = id2label[prediction]
        
        if label.startswith('I-'):
            current_entity += ' ' + token
        elif label.startswith('B-'):
            if current_entity:
                entities.append(current_entity.strip())
            current_entity = token
    
    if current_entity:
        entities.append(current_entity.strip())
    
    return entities


# Path to the combined text file
txt_file_path = '/Users/justincooper/Desktop/Alireza Code/Output/combined_text.txt'

# Read the text from the file
with open(txt_file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# Extract entities using spaCy
diseases_spacy = extract_entities_spacy(text, nlp_sci)
drugs_spacy = extract_entities_spacy(text, nlp_bc5cdr)

# Extract entities using BioBERT
diseases_biobert = extract_entities_biobert(text, biobert_tokenizer, biobert_model)
drugs_biobert = extract_entities_biobert(text, biobert_tokenizer, biobert_model)

# Compare the differences
common_diseases = set(diseases_spacy) & set(diseases_biobert)
common_drugs = set(drugs_spacy) & set(drugs_biobert)

print(f"Total diseases detected by spaCy: {len(diseases_spacy)}")
print(f"Total diseases detected by BioBERT: {len(diseases_biobert)}")
print(f"Total drugs detected by spaCy: {len(drugs_spacy)}")
print(f"Total drugs detected by BioBERT: {len(drugs_biobert)}")

print(f"Common diseases: {common_diseases}")
print(f"Differences in diseases (spaCy - BioBERT): {set(diseases_spacy) - set(diseases_biobert)}")

print(f"Common drugs: {common_drugs}")
print(f"Differences in drugs (spaCy - BioBERT): {set(drugs_spacy) - set(drugs_biobert)}")

# Check for most common words
word_counts_spacy = Counter(text.split())
word_counts_biobert = Counter(text.split())

top_10_words_spacy = word_counts_spacy.most_common(10)
top_10_words_biobert = word_counts_biobert.most_common(10)

print(f"Top 10 most common words with spaCy: {top_10_words_spacy}")
print(f"Top 10 most common words with BioBERT: {top_10_words_biobert}")
